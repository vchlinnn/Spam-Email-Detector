---
title: "hw9_nhap"
output: pdf_document
date: "2024-04-22"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### The Data

The data set `spam_train` can be found in the hw_9 repo and can be loaded by running the following code.

```{r message =F}
spam_train<-read.csv("spam_train.csv")
spam_train$spam <- as.factor(spam_train$spam)
```

```{r}
str(spam_train)
```


- `spam` is your response variable (coded as a factor variable, where 1 = spam and 0 = not spam) and should not be included as a predictor.




## Components


### Data Exploration

In this section, you should perform preliminary data exploration and analysis. This data set is a bit too large to do a full investigation of each variable, so select several variables you think may be useful (at least 6 for visualization purposes, but you may want to use many more for the actual model building). Create visualizations for each variable individually, along with visualizations showing the relationship between the variable and the response. Compute relevant summary statistics for each of your variables.

******************************************

```{r}
# Convert spam to a numerical variable to compute correlation matrix 
spam_train$spam_num <- ifelse(spam_train$spam=="1", 1, 0)
str(spam_train)
```

```{r}
# Compute correlation matrix and filter the predictors that have the strongest correlation
library(dplyr)
correlation <- cor(spam_train[,-58], spam_train$spam_num)
cor_dataframe <- data.frame(variable = rownames (correlation), cor = as.numeric(correlation))
cor_dataframe %>% filter(abs(cor) > 0.25)
```
```{r}
correlation
```


```{r}
cor(spam_train[c("word_freq_our", "word_freq_over", "word_freq_remove", "word_freq_you", "word_freq_000", 
                 "char_freq_..3", "capital_run_length_longest", "word_freq_hp", "word_freq_receive", "word_freq_business",    "word_freq_hp.1")])
```

```{r}
spam_train$spam <- as.factor(spam_train$spam)
spam_train$spam <- factor(spam_train$spam,levels = c("1","0"))
```

word_freq_hp and word_freq_hp.1 

```{r}
library(ggplot2)
# Interesting pattern: word_freq_hp and word_freq_business make a good pair 
# (Notice that they have opposite correlation)

# word_freq_hp is imbalanced towards spam 1 --> good 
ggplot(data=spam_train) + geom_boxplot(mapping= aes(x=spam, y=word_freq_hp))

# word_freq_business is imbalanced towards spam 2 --> good 
ggplot(data=spam_train) + geom_boxplot(mapping= aes(x=spam, y=word_freq_business))


# combine both
ggplot(spam_train, aes(x=word_freq_hp, y=word_freq_business)) +
geom_point(aes(color=spam))
```
```{r}
ggplot(spam_train, aes(x=word_freq_hp.1, y=word_freq_receive)) +
geom_point(aes(color=spam))
```

```{r}
# capital_run_length_longest fair 
ggplot(data=spam_train) + geom_boxplot(mapping= aes(x=spam, y=capital_run_length_longest))
```

```{r}
# word_freq_000 fair enough 
ggplot(data=spam_train) + geom_boxplot(mapping= aes(x=spam, y=word_freq_000))
```

ggplot(Auto, aes(x=weight, y=horsepower)) +
geom_point(aes(color=high_mpg))

```{r}
#word_freq_you: fair enough 
ggplot(data=spam_train) + geom_boxplot(mapping= aes(x=spam, y=word_freq_you))
```
```{r}
# Too skewed and the levels of spam are divided equally so we cannot tell 

# word_freq_our is too skewed 
ggplot(spam_train, aes(x = word_freq_our)) +
  geom_histogram(binwidth = 1) +
  labs(x = "word_freq_our", y = "Frequency", title = "Histogram of word_freq_our") +
  theme_minimal()

# char_freq_..3 is too skewed 
ggplot(spam_train, aes(x = char_freq_..3)) +
  geom_histogram(binwidth = 1) +
  labs(x = "word_freq_our", y = "Frequency", title = "Histogram of word_freq_our") +
  theme_minimal()
```



******************************************

### Model Building

In this section, you should build a series of models (at least 5, but more is probably better) of varying complexity and that use a variety of the tools we have studied thus far. Explain why you choose to implement various features in each model. Here are some suggestions:

  - Build at least one model using a large number of variables
  
  - Build at least one model using a small number of variables
  
  - Build at least one highly flexible model
  
  - Build at least one highly rigid model
  
  - Build at least one model that has a parameter that can be tuned using cross-validation
  
  - Consider models using a variety of classification threshold (i.e. the value of $c$ in the rule "Predict $Y = 1$ if $P(Y = 1 | X ) > c$")




******************************************

We can see the group of 4 predictors with opposite correlation. I think that KNN might work well with this because they are separate positions. 

```{r}
library(ISLR)
library(tidyr)
library(rsample)
library(purrr)
library(modelr)  
library(yardstick)  
library(dplyr)
# Create 10 folds for cross validation 
set.seed(200)
my_cv <- vfold_cv(spam_train, v = 10)

# Fit KNN on the training data to predict high_mpg
library(kknn)
spam_knn <- function(k, split){
  model <- kknn(spam ~ word_freq_hp.1 + word_freq_hp + word_freq_receive + word_freq_business, 
                train = analysis(split), 
                test = assessment(split),
                distance = 1,
                k = k, 
                kernel = "rectangular")
  test = assessment(split)
  my_preds <- model$fitted.values
  my_obs <- test$spam
  accuracy <- sum(my_obs==my_preds)/nrow(test)
  error <- 1 - accuracy
  error
}

# Use cross-validation on the training set to select the optimal value of k
library(purrr)
my_error_mat <- as.data.frame(matrix(NA, ncol = 30, nrow = 10))
for (i in 1:30){
my_error_mat[,i] <- map_dbl(my_cv$splits, spam_knn, k = i)
}

# Edit column names 
colnames(my_error_mat) <- paste("knn",1:30)

# Create plot
knn_error <- map_dbl(my_error_mat, mean)
data.frame(k=1:30, knn_error) %>% ggplot(aes(x=k, y=knn_error)) + geom_line() + geom_point()

```
I would go with k = 5.

```{r}
# My first model
spam_knn_mod1 <- function(split){
  model <- kknn(spam ~ word_freq_hp.1 + word_freq_hp + word_freq_receive + word_freq_business, 
                train = analysis(split), 
                test = assessment(split),
                distance = 1,
                k = 5, 
                kernel = "rectangular")
  test = assessment(split)
  my_preds <- model$fitted.values
  my_obs <- test$spam
  accuracy <- sum(my_obs==my_preds)/nrow(test)
  error <- 1 - accuracy
  error
}

my_error_mat <- as.data.frame(matrix(NA, ncol = 1, nrow = 10))
my_error_mat[,1] <- map_dbl(my_cv$splits, spam_knn_mod1)
mean(my_error_mat[,1])

```

```{r}
# Fit a logistic regression model on the training data to predict `high_mpg`
spam_logistic_mod <- function(split){
  model <- glm(spam ~.-spam_num, data = analysis(split), family = "binomial")
  test = assessment(split)
  my_probs <- predict(model, newdata = test, type = "response")
  my_preds <- ifelse(my_probs > 0.5, "0","1")
  my_obs <- test$spam
  accuracy <- sum(my_obs == my_preds)/nrow(test)
}
  
my_error_mat <- as.data.frame(matrix(NA, ncol = 1, nrow = 10))
my_error_mat[,1] <- map_dbl(my_cv$splits, spam_logistic_mod)
mean(my_error_mat[,1])
```
```{r}
# SPECIFICITY MODEL 

spam_logistic_mod <- function(split) {
  model <- glm(spam ~ .-spam_num, 
               data = analysis(split), family = "binomial")
  test <- assessment(split)
  my_probs <- predict(model, newdata = test, type = "response")
  my_preds <- ifelse(my_probs > 0.9, "1", "0")
  my_obs <- factor(test$spam, levels = c("0", "1"))
  my_preds <- factor(my_preds, levels = c("0", "1"))
  results <- data.frame(truth = my_obs, estimate = my_preds)
  conf_matrix <- conf_mat(results, truth = truth, estimate = estimate)
  matrix_counts <- conf_matrix$table
  tp <- matrix_counts[1, 1]
  fp <- matrix_counts[1, 2]
  fn <- matrix_counts[2, 1]
  tn <- matrix_counts[2, 2]
  accuracy <- (tp + tn) / sum(matrix_counts)
  sensitivity <- tp / (tp + fn)
  specificity <- tn / (tn + fp)
  c(Accuracy = 1 - accuracy, Sensitivity = 1 - as.numeric(sensitivity), Specificity = 1 - as.numeric(specificity))
}

# Use map_dfr to apply the function to each split and get a data frame
my_error_mat <- map_dfr(my_cv$splits, spam_logistic_mod)

# Calculate the mean for each column
mean_metrics <- colMeans(my_error_mat)
mean_metrics
```

```{r}
# SENSITIVITY MODEL 

spam_logistic_mod <- function(split) {
  model <- glm(spam ~ .-spam_num, 
               data = analysis(split), family = "binomial")
  test <- assessment(split)
  my_probs <- predict(model, newdata = test, type = "response")
  my_preds <- ifelse(my_probs > 0.1, "1", "0")
  my_obs <- factor(test$spam, levels = c("0", "1"))
  my_preds <- factor(my_preds, levels = c("0", "1"))
  results <- data.frame(truth = my_obs, estimate = my_preds)
  conf_matrix <- conf_mat(results, truth = truth, estimate = estimate)
  matrix_counts <- conf_matrix$table
  tp <- matrix_counts[1, 1]
  fp <- matrix_counts[1, 2]
  fn <- matrix_counts[2, 1]
  tn <- matrix_counts[2, 2]
  accuracy <- (tp + tn) / sum(matrix_counts)
  sensitivity <- tp / (tp + fn)
  specificity <- tn / (tn + fp)
  c(Accuracy = 1 - accuracy, Sensitivity = 1 - as.numeric(sensitivity), Specificity = 1 - as.numeric(specificity))
}

# Use map_dfr to apply the function to each split and get a data frame
my_error_mat <- map_dfr(my_cv$splits, spam_logistic_mod)

# Calculate the mean for each column
mean_metrics <- colMeans(my_error_mat)
mean_metrics
```
```{r}
spam_knn_mod1 <- function(split){
  model <- kknn(spam ~ word_freq_hp.1 + word_freq_hp + word_freq_receive + word_freq_business, 
                train = analysis(split), 
                test = assessment(split),
                distance = 1,
                k = 5, 
                kernel = "rectangular")
  test = assessment(split)
  my_preds <- model$fitted.values
  my_obs <- factor(test$spam, levels = c("0", "1"))
  my_preds <- factor(my_preds, levels = c("0", "1"))
  results <- data.frame(truth = my_obs, estimate = my_preds)
  conf_matrix <- conf_mat(results, truth = truth, estimate = estimate)
  matrix_counts <- conf_matrix$table
  tp <- matrix_counts[1, 1]
  fp <- matrix_counts[1, 2]
  fn <- matrix_counts[2, 1]
  tn <- matrix_counts[2, 2]
  accuracy <- (tp + tn) / sum(matrix_counts)
  sensitivity <- tp / (tp + fn)
  specificity <- tn / (tn + fp)
  c(Accuracy = accuracy, Sensitivity = as.numeric(sensitivity), Specificity = as.numeric(specificity))
}

# Use map_dfr to apply the function to each split and get a data frame
my_error_mat <- map_dfr(my_cv$splits, spam_knn_mod1)

# Calculate the mean for each column
mean_metrics <- colMeans(my_error_mat)
mean_metrics
```
```{r}
library(e1071)

spam_naive_mod <- function(split) {
  model <- naiveBayes(spam ~ .-spam_num, 
               data = analysis(split))
  test <- assessment(split)
  my_preds <- predict(model, test)
  my_probs <- predict(model, test, type = "raw")
  my_obs <- factor(test$spam, levels = c("0", "1"))
  my_preds <- factor(my_preds, levels = c("0", "1"))
  results <- data.frame(truth = my_obs, estimate = my_preds)
  conf_matrix <- conf_mat(results, truth = truth, estimate = estimate)
  matrix_counts <- conf_matrix$table
  tp <- matrix_counts[1, 1]
  fp <- matrix_counts[1, 2]
  fn <- matrix_counts[2, 1]
  tn <- matrix_counts[2, 2]
  accuracy <- (tp + tn) / sum(matrix_counts)
  sensitivity <- tp / (tp + fn)
  specificity <- tn / (tn + fp)
  c(Accuracy = accuracy, Sensitivity = as.numeric(sensitivity), Specificity = as.numeric(specificity))
}

# Use map_dfr to apply the function to each split and get a data frame
my_error_mat <- map_dfr(my_cv$splits, spam_naive_mod)

# Calculate the mean for each column
mean_metrics <- colMeans(my_error_mat)
mean_metrics
```
```{r}
set.seed(1)
library(rpart)

spam_tree_mod <- function(split) {
  model <- rpart(spam ~ .-spam_num,
                 data = analysis(split))
  test <- assessment(split)
  my_obs <- factor(test$spam, levels = c("0", "1"))
  my_preds <- predict(model, test, type = "class")
  my_preds <- factor(my_preds, levels = c("0", "1"))
  my_probs <- predict(model, test, type = "prob")[,"spam"]
  results <- data.frame(probs = my_probs,
                        truth = my_obs,
                        estimate = my_preds)
  conf_matrix <- conf_mat(results, truth = my_obs, estimate = my_preds)
  matrix_counts <- conf_matrix$table
  tp <- matrix_counts[1, 1]
  fp <- matrix_counts[1, 2]
  fn <- matrix_counts[2, 1]
  tn <- matrix_counts[2, 2]
  accuracy <- (tp + tn) / sum(matrix_counts)
  sensitivity <- tp / (tp + fn)
  specificity <- tn / (tn + fp)
  c(Accuracy = accuracy, Sensitivity = as.numeric(sensitivity), Specificity = as.numeric(specificity))
}

# Use map_dfr to apply the function to each split and get a data frame
my_error_mat <- map_dfr(my_cv$splits, spam_tree_mod)

# Calculate the mean for each column
mean_metrics <- colMeans(my_error_mat)
mean_metrics
```
```{r}
# only -word_freq_will 0.9336359   0.9513631   0.9051629
# -spam_num -word_freq_will -word_freq_parts -word_freq_address -word_freq_3d -word_freq_mail -word_freq_people 
# 0.9314620   0.9585793   0.8877579
# -spam_num -word_freq_will -word_freq_parts -word_freq_address -word_freq_3d -word_freq_mail -word_freq_people -word_freq_report -word_freq_addresses
# ntree 20 mtry 10 
# 0.9390946   0.9587656   0.9063168
# -spam_num -word_freq_parts -word_freq_will -word_freq_table -char_freq_..5 -word_freq_report
# ntree 20 mtry 10
# 0.9379838   0.9611018   0.9016343

set.seed(12)
library(randomForest)

spam_rforest_mod <- function(split) {
  model <- randomForest(spam ~ .-spam_num -word_freq_table -word_freq_report -word_freq_3d -word_freq_address,
                 data = analysis(split), 
                 ntree = 20, mtry = 10)
  test <- assessment(split)
  my_preds <- predict(model, test, type = "class")
  my_obs <- factor(test$spam, levels = c("0", "1"))
  my_preds <- factor(my_preds, levels = c("0", "1"))
  results <- data.frame(truth = my_obs,
                        estimate = my_preds)
  conf_matrix <- conf_mat(results, truth = truth, estimate = estimate)
  matrix_counts <- conf_matrix$table
  tp <- matrix_counts[1, 1]
  fp <- matrix_counts[1, 2]
  fn <- matrix_counts[2, 1]
  tn <- matrix_counts[2, 2]
  accuracy <- (tp + tn) / sum(matrix_counts)
  sensitivity <- tp / (tp + fn)
  specificity <- tn / (tn + fp)
  c(Accuracy = accuracy, Sensitivity = as.numeric(sensitivity), Specificity = as.numeric(specificity))
}

# Use map_dfr to apply the function to each split and get a data frame
my_error_mat <- map_dfr(my_cv$splits, spam_rforest_mod)

# Calculate the mean for each column
mean_metrics <- colMeans(my_error_mat)
mean_metrics
```
```{r}
# Fit a logistic regression model on the training data to predict `high_mpg`
spam_logistic_mod <- function(split){
  model <- glm(spam ~.-spam_num -word_freq_will -word_freq_report, 
               data = analysis(split), family = "binomial")
  test = assessment(split)
  my_probs <- predict(model, newdata = test, type = "response")
  my_preds <- ifelse(my_probs > 0.5, "0","1")
  my_obs <- test$spam
  accuracy <- sum(my_obs == my_preds)/nrow(test)
}
  
my_error_mat <- as.data.frame(matrix(NA, ncol = 1, nrow = 10))
my_error_mat[,1] <- map_dbl(my_cv$splits, spam_logistic_mod)
mean(my_error_mat[,1])
```





******************************************

### Model Selection

In this section, evaluate model performance using a variety of metrics. Which models seemed to perform better or worse? Why? Here are some suggestions:

  - Use cross-validation as well as training + test sets to evaluate performance
  
  - After assessing performance, revisit models and make small changes
  
  - Consider the structure of the predictors. What relationships do these predictors have? What types of models will tend to work best for these relationships?
  
  - Consider modifications that can be made to increase either sensitivity or specificity.

******************************************





******************************************

### Your model

**Goals**: Identify the three models you feel will best satisfy the following goals:


1. The model with the highest overall accuracy (at least 90%)

2. The model with the highest specificity while maintaining reasonable accuracy (specificity at least 95%, accuracy at least 80%)

3. The model with the highest sensitivity while maintaining reasonable accuracy (sensitivity at least 95%, accuracy at least 80%)


Load the evaluation data using the following code:

```{r}
spam_eval <-read.csv("spam_eval.csv")
```



Use your 3 models to make 3 sets of predictions on `spam_eval`:

1. Make predictions using the model that best achieves goal 1. Save your predictions as the data frame called `FirstName_LastName_goal1`

2. Make predictions using the model that best achieves goal 2. Save your predictions as the data frame called `FirstName_LastName_goal2`

3. Make predictions using the model that best achieves goal 3. Save your predictions as the data frame called `FirstName_LastName_goal3`

*Be sure to replace `FirstName_LastName` with your actual first and last names in the above data frame names.*

******************************************





******************************************




Once you have made predictions, remove the # symbol from the following lines of code, and replace `FirstName_LastName` with your actual first and last name. **Do not delete the letters .csv at the end of the file name inside the quotations.** Then run the code. This will save your data frames as .csv files in your repo. 

```{r}
# write.csv(FirstName_LastName_goal1, "FirstName_LastName_goal1.csv", row_names = F)
# write.csv(FirstName_LastName_goal2, "FirstName_LastName_goal2.csv", row_names = F)
# write.csv(FirstName_LastName_goal3, "FirstName_LastName_goal3.csv", row_names = F)
```

*Be sure you commit and push these .csv files to the Github repo in addition to your .Rmd, as they are the predictions I will use to evaluate your model.*

### Conclusions

Discuss some limitations of your methods and your model. What are some ways you could improve your model if you had more **time**? Identify one variable **not** in the data set you feel could be an important predictor of `spam`. How confident are you in the accuracy of your model? 

******************************************





******************************************