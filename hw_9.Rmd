---
title: "Homework 9"
author: "Linh Vu"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Instructions

1. Add your name between the quotations marks on the author line in the YAML above.

2. Work through the problems below, composing your answer to each question between the bars of red stars.

3. Be sure to make commits often and push your commits to Github periodically.

4. When finished, knit your document to .pdf and view the resulting output to ensure it matches your intent.

5. To submit your assignment, make one final commit. Then make push your repo with all commits back to Github one final time.

## Classification Competition 


Your objective is to build a classification model **using the tools we have discussed in class** that predicts whether an email is spam, based on other characteristics of the email. You will construct your model using a training data set with information on 57 variables recorded for 919 emails. I've also included an evaluation set consisting of the predictor values (but not the responses) for 309 additional emails. You will use your models to make predictions on these additional emails, which I will compare to their true spam status (that I have recorded, but have withheld) in order to evaluate your model.

You should record your answers in this .Rmd file. However, you are encouraged to use a separate .Rmd file for scratchwork. You will be asked to reflect on your thought process, setbacks, and successes, as well as the code you included in this homework, during our second [Code Review](https://grinnell-sta-295-s24.github.io/syllabus.html#Code_Review) week 13, so be sure to keep detailed notes of your investigation.

The assignment is divided into several **Components** to help organize your work. Put all work you want graded between the bars of red stars in the corresponding section.

### Grading

Your final score on this assignment will be based 15% on the **performance** of your model predictions on the evaluation data, as measured by accuracy, sensitivity and specificity (as detailed below) and 85% on the quality and depth of your explanations and analysis. 

I *do not* expect you to find the absolute best model for this data set. In fact, it is entirely possible to earn top marks on this assignment with a model of mediocre accuracy, provided you submit insightful analysis based on topics we have investigated in our course.

You will be graded as much on your discussion of what *didn't work* and why, as what did.

### Goal

At the end of this assignment, you will construct 3 models that you feel best achieve each of the following goals. I've also included benchmarks to help you assess whether you've achieved these goals:

1. The model with the highest overall accuracy (at least 90%)

2. The model with the highest specificity while maintaining reasonable accuracy (specificity at least 95%, accuracy at least 80%)

3. The model with the highest sensitivity while maintaining reasonable accuracy (sensitivity at least 95%, accuracy at least 80%)

Achieving each of these goals will earn at least a B+ on the "performance" component of the assignment. 

### The Data

The data set `spam_train` can be found in the hw_9 repo and can be loaded by running the following code.

```{r message =F}
spam_train<-read.csv("spam_train.csv")
spam_train$spam <- as.factor(spam_train$spam)
```

Additionally, the `data_description.txt` file in the same repo gives a full description of the variables appearing in the data set.

There is one special column of note: 

- `spam` is your response variable (coded as a factor variable, where 1 = spam and 0 = not spam) and should not be included as a predictor.




## Components


### Data Exploration

In this section, you should perform preliminary data exploration and analysis. This data set is a bit too large to do a full investigation of each variable, so select several variables you think may be useful (at least 6 for visualization purposes, but you may want to use many more for the actual model building). Create visualizations for each variable individually, along with visualizations showing the relationship between the variable and the response. Compute relevant summary statistics for each of your variables.

******************************************
```{r}
# Load the necessary libraries
library(dplyr)
library(ggplot2)
```


To perform preliminary data exploration and analysis, we will first look at the structure of the data set:

```{r}
str(spam_train)
```
The data contains 919 observations of 58 variables. Our response variable is a binary categorical variable whose levels encoded as "0" and "1". One noteworthy fact is that all of our predictors are numerical. Thus, I would want to create `spam_num`, which is the numerical version of my response variable to compute a correlation matrix among the variables. 

```{r}
# Convert spam to a numerical variable to compute correlation matrix 
spam_train$spam_num <- ifelse(spam_train$spam=="1", 1, 0)

# Compute correlation matrix and filter the predictors that have the strongest correlation with `spam_num` (which is > 0.25 in this case)
correlation <- cor(spam_train[,-58], spam_train$spam_num)
cor_dataframe <- data.frame(variable = rownames(correlation), cor = as.numeric(correlation))
cor_dataframe %>% filter(abs(cor) > 0.25)
```
We would pay more attention to these predictors by creating data visualizations. I first noticed that some of these predictors have opposite correlation with the response, such as `word_freq_hp.1` (-0.2597231) and `word_freq_receive` (0.2563150), or `word_freq_hp` (-0.2651063) and `word_freq_business` (0.2670211). We can visualize this correlation through box plots: 

```{r}
# word_freq_hp is imbalanced towards spam 0
ggplot(data=spam_train) + geom_boxplot(mapping= aes(x=spam, y=word_freq_hp))
```
```{r}
# word_freq_business is imbalanced towards spam 1
ggplot(data=spam_train) + geom_boxplot(mapping= aes(x=spam, y=word_freq_business))
```
These pairs of predictors might be helpful in classifying whether or not an email is a spam. Let's confirm this belief by visualizing each pairs of predictors with opposite correlation:

```{r}
# Scatterplot of word_freq_hp, word_freq_business, and spam coded as color 
ggplot(spam_train, aes(x=word_freq_hp, y=word_freq_business)) +
geom_point(aes(color=spam))
```
```{r}
# Scatterplot of word_freq_hp.1, word_freq_receive, and spam coded as color 
ggplot(spam_train, aes(x=word_freq_hp.1, y=word_freq_receive)) +
geom_point(aes(color=spam))
```
By knowing the values of these predictors, we would gain much information about whether or not an email is a spam.

We would want to avoid predictors with too imbalanced observations among their levels, as the model might become biased towards the more frequent levels. However, there are some predictors with slightly imbalanced observations yet different range for spam and not spam emails such as `word_freq_000` and `capital_run_length_longest`.

```{r}
# Box plot of spam and capital_run_length_longest 
ggplot(data=spam_train) + geom_boxplot(mapping= aes(x=spam, y=capital_run_length_longest))
```
If the value of `capital_run_length_longest` is above 600 units, we would much more likely to classify it as a spam email.

```{r}
# Box plot of spam and word_freq_000
ggplot(data=spam_train) + geom_boxplot(mapping= aes(x=spam, y=word_freq_000))
```
Likewise, if the value of `word_freq_000` is above 2, we would much more likely to classify it as a spam email.


******************************************

### Model Building

In this section, you should build a series of models (at least 5, but more is probably better) of varying complexity and that use a variety of the tools we have studied thus far. Explain why you choose to implement various features in each model. Here are some suggestions:

  - Build at least one model using a large number of variables
  
  - Build at least one model using a small number of variables
  
  - Build at least one highly flexible model
  
  - Build at least one highly rigid model
  
  - Build at least one model that has a parameter that can be tuned using cross-validation
  
  - Consider models using a variety of classification threshold (i.e. the value of $c$ in the rule "Predict $Y = 1$ if $P(Y = 1 | X ) > c$")




******************************************





******************************************

### Model Selection

In this section, evaluate model performance using a variety of metrics. Which models seemed to perform better or worse? Why? Here are some suggestions:

  - Use cross-validation as well as training + test sets to evaluate performance
  
  - After assessing performance, revisit models and make small changes
  
  - Consider the structure of the predictors. What relationships do these predictors have? What types of models will tend to work best for these relationships?
  
  - Consider modifications that can be made to increase either sensitivity or specificity.

******************************************





******************************************

### Your model

**Goals**: Identify the three models you feel will best satisfy the following goals:


1. The model with the highest overall accuracy (at least 90%)

2. The model with the highest specificity while maintaining reasonable accuracy (specificity at least 95%, accuracy at least 80%)

3. The model with the highest sensitivity while maintaining reasonable accuracy (sensitivity at least 95%, accuracy at least 80%)


Load the evaluation data using the following code:

```{r}
spam_eval <-read.csv("spam_eval.csv")
```



Use your 3 models to make 3 sets of predictions on `spam_eval`:

1. Make predictions using the model that best achieves goal 1. Save your predictions as the data frame called `FirstName_LastName_goal1`

2. Make predictions using the model that best achieves goal 2. Save your predictions as the data frame called `FirstName_LastName_goal2`

3. Make predictions using the model that best achieves goal 3. Save your predictions as the data frame called `FirstName_LastName_goal3`

*Be sure to replace `FirstName_LastName` with your actual first and last names in the above data frame names.*

******************************************





******************************************




Once you have made predictions, remove the # symbol from the following lines of code, and replace `FirstName_LastName` with your actual first and last name. **Do not delete the letters .csv at the end of the file name inside the quotations.** Then run the code. This will save your data frames as .csv files in your repo. 

```{r}
# write.csv(FirstName_LastName_goal1, "FirstName_LastName_goal1.csv", row_names = F)
# write.csv(FirstName_LastName_goal2, "FirstName_LastName_goal2.csv", row_names = F)
# write.csv(FirstName_LastName_goal3, "FirstName_LastName_goal3.csv", row_names = F)
```

*Be sure you commit and push these .csv files to the Github repo in addition to your .Rmd, as they are the predictions I will use to evaluate your model.*

### Conclusions

Discuss some limitations of your methods and your model. What are some ways you could improve your model if you had more **time**? Identify one variable **not** in the data set you feel could be an important predictor of `spam`. How confident are you in the accuracy of your model? 

******************************************





******************************************